{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_probability as tfp\n",
    "from PPO.model import Model\n",
    "import numpy as np\n",
    "import gym\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: tensorflow_probability in /opt/conda/lib/python3.7/site-packages (0.13.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_probability) (1.15.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from tensorflow_probability) (4.4.2)\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.7/site-packages (from tensorflow_probability) (0.1.6)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow_probability) (1.4.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow_probability) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow_probability) (1.19.5)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE_PATH = './PPOCartpole'\n",
    "CRITIC_LOSS_WEIGHT = 0.5\n",
    "ENTROPY_LOSS_WEIGHT = 0.01\n",
    "ENT_DISCOUNT_RATE = 0.995\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "CLIP_VALUE = 0.2\n",
    "LR = 0.001\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "state_size = 4\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "ent_discount_val = ENTROPY_LOSS_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_loss(discounted_rewards, value_est):\n",
    "    return tf.cast(tf.reduce_mean(keras.losses.mean_squared_error(discounted_rewards, value_est)) * CRITIC_LOSS_WEIGHT,\n",
    "                   tf.float32)\n",
    "\n",
    "\n",
    "def entropy_loss(policy_logits, ent_discount_val):\n",
    "    probs = tf.nn.softmax(policy_logits)\n",
    "    entropy_loss = -tf.reduce_mean(keras.losses.categorical_crossentropy(probs, probs))\n",
    "    return entropy_loss * ent_discount_val\n",
    "\n",
    "\n",
    "def actor_loss(advantages, old_probs, action_inds, policy_logits):\n",
    "    probs = tf.nn.softmax(policy_logits)\n",
    "    new_probs = tf.gather_nd(probs, action_inds)\n",
    "\n",
    "    ratio = new_probs / old_probs\n",
    "\n",
    "    policy_loss = -tf.reduce_mean(tf.math.minimum(\n",
    "        ratio * advantages,\n",
    "        tf.clip_by_value(ratio, 1.0 - CLIP_VALUE, 1.0 + CLIP_VALUE) * advantages\n",
    "    ))\n",
    "    return policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(action_inds, old_probs, states, advantages, discounted_rewards, optimizer, ent_discount_val):\n",
    "    with tf.GradientTape() as tape:\n",
    "        values, policy_logits = model.call(tf.stack(states))\n",
    "        act_loss = actor_loss(advantages, old_probs, action_inds, policy_logits)\n",
    "        ent_loss = entropy_loss(policy_logits, ent_discount_val)\n",
    "        c_loss = critic_loss(discounted_rewards, values)\n",
    "        tot_loss = act_loss + ent_loss + c_loss\n",
    "    grads = tape.gradient(tot_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return tot_loss, c_loss, act_loss, ent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advantages(rewards, dones, values, next_value):\n",
    "    discounted_rewards = np.array(rewards + [next_value[0]])\n",
    "\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        discounted_rewards[t] = rewards[t] + GAMMA * discounted_rewards[t+1] * (1-dones[t])\n",
    "    discounted_rewards = discounted_rewards[:-1]\n",
    "    # advantages are bootstrapped discounted rewards - values, using Bellman's equation\n",
    "    advantages = discounted_rewards - np.stack(values)[:, 0]\n",
    "    # standardise advantages\n",
    "    advantages -= np.mean(advantages)\n",
    "    advantages /= (np.std(advantages) + 1e-10)\n",
    "    # standardise rewards too\n",
    "    discounted_rewards -= np.mean(discounted_rewards)\n",
    "    discounted_rewards /= (np.std(discounted_rewards) + 1e-8)\n",
    "    return discounted_rewards, advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_actions)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.create_file_writer(STORE_PATH + f\"/PPO-CartPole_{dt.datetime.now().strftime('%d%m%Y%H%M')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100000\n",
    "episode_reward_sum = 0\n",
    "state = env.reset()\n",
    "episode = 1\n",
    "total_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4, latest episode reward: 18.0, total loss: 0.5638254463672638, critic loss: 0.5834591448307037, actor loss: -0.015202110446989537, entropy loss -0.004431585595011711\n",
      "Episode: 5, latest episode reward: 25.0, total loss: 0.5638254463672638, critic loss: 0.5834591448307037, actor loss: -0.015202110446989537, entropy loss -0.004431585595011711\n",
      "Episode: 6, latest episode reward: 19.0, total loss: 0.5638254463672638, critic loss: 0.5834591448307037, actor loss: -0.015202110446989537, entropy loss -0.004431585595011711\n",
      "Episode: 7, latest episode reward: 10.0, total loss: 0.5638254463672638, critic loss: 0.5834591448307037, actor loss: -0.015202110446989537, entropy loss -0.004431585595011711\n",
      "Episode: 8, latest episode reward: 22.0, total loss: 0.4826167196035385, critic loss: 0.5108057141304017, actor loss: -0.02265058271586895, entropy loss -0.005538411810994149\n",
      "Episode: 9, latest episode reward: 26.0, total loss: 0.4826167196035385, critic loss: 0.5108057141304017, actor loss: -0.02265058271586895, entropy loss -0.005538411810994149\n",
      "Episode: 10, latest episode reward: 41.0, total loss: 0.4945587426424026, critic loss: 0.5020328342914582, actor loss: -0.0013632601127028464, entropy loss -0.006110825762152672\n",
      "Episode: 11, latest episode reward: 23.0, total loss: 0.4945587426424026, critic loss: 0.5020328342914582, actor loss: -0.0013632601127028464, entropy loss -0.006110825762152672\n",
      "Episode: 12, latest episode reward: 42.0, total loss: 0.4876046270132065, critic loss: 0.5037730038166046, actor loss: -0.009737801179289818, entropy loss -0.006430573528632522\n",
      "Episode: 13, latest episode reward: 15.0, total loss: 0.4876046270132065, critic loss: 0.5037730038166046, actor loss: -0.009737801179289818, entropy loss -0.006430573528632522\n",
      "Episode: 14, latest episode reward: 105.0, total loss: 0.5024652302265167, critic loss: 0.5341411292552948, actor loss: -0.025280440226197243, entropy loss -0.006395464250817895\n",
      "Episode: 15, latest episode reward: 23.0, total loss: 0.5024652302265167, critic loss: 0.5341411292552948, actor loss: -0.025280440226197243, entropy loss -0.006395464250817895\n",
      "Episode: 16, latest episode reward: 9.0, total loss: 0.5024652302265167, critic loss: 0.5341411292552948, actor loss: -0.025280440226197243, entropy loss -0.006395464250817895\n",
      "Episode: 17, latest episode reward: 48.0, total loss: 0.4913258492946625, critic loss: 0.5123663663864135, actor loss: -0.014705559983849526, entropy loss -0.0063349577132612465\n",
      "Episode: 18, latest episode reward: 37.0, total loss: 0.49224519431591035, critic loss: 0.5022308170795441, actor loss: -0.00391642153263092, entropy loss -0.0060692015569657086\n",
      "Episode: 19, latest episode reward: 21.0, total loss: 0.49224519431591035, critic loss: 0.5022308170795441, actor loss: -0.00391642153263092, entropy loss -0.0060692015569657086\n",
      "Episode: 20, latest episode reward: 88.0, total loss: 0.49217074513435366, critic loss: 0.5023900866508484, actor loss: -0.0041538413614034654, entropy loss -0.0060654959641397\n",
      "Episode: 21, latest episode reward: 29.0, total loss: 0.49630037844181063, critic loss: 0.5064312517642975, actor loss: -0.003689903113991022, entropy loss -0.006440971978008747\n",
      "Episode: 22, latest episode reward: 39.0, total loss: 0.49630037844181063, critic loss: 0.5064312517642975, actor loss: -0.003689903113991022, entropy loss -0.006440971978008747\n",
      "Episode: 23, latest episode reward: 26.0, total loss: 0.491436105966568, critic loss: 0.5018446505069732, actor loss: -0.0043324345722794535, entropy loss -0.006076106429100036\n",
      "Episode: 24, latest episode reward: 32.0, total loss: 0.491436105966568, critic loss: 0.5018446505069732, actor loss: -0.0043324345722794535, entropy loss -0.006076106429100036\n",
      "Episode: 25, latest episode reward: 83.0, total loss: 0.4929508358240128, critic loss: 0.5008777797222137, actor loss: -0.0021999018266797067, entropy loss -0.005727038765326142\n",
      "Episode: 26, latest episode reward: 42.0, total loss: 0.4929508358240128, critic loss: 0.5008777797222137, actor loss: -0.0021999018266797067, entropy loss -0.005727038765326142\n",
      "Episode: 27, latest episode reward: 157.0, total loss: 0.4960790604352951, critic loss: 0.5066645026206971, actor loss: -0.0052707578055560585, entropy loss -0.005314682377502322\n",
      "Episode: 28, latest episode reward: 134.0, total loss: 0.4764477789402008, critic loss: 0.5016665875911712, actor loss: -0.019890035316348077, entropy loss -0.005328777525573969\n",
      "Episode: 29, latest episode reward: 110.0, total loss: 0.4912991613149643, critic loss: 0.5005682051181793, actor loss: -0.0033253783360123633, entropy loss -0.005943666538223624\n",
      "Episode: 30, latest episode reward: 38.0, total loss: 0.48542594313621523, critic loss: 0.5023561239242553, actor loss: -0.011458360590040684, entropy loss -0.0054718208964914085\n",
      "Episode: 31, latest episode reward: 99.0, total loss: 0.4808022230863571, critic loss: 0.5013214170932769, actor loss: -0.015466632042080163, entropy loss -0.005052563734352589\n",
      "Episode: 32, latest episode reward: 95.0, total loss: 0.49436472058296205, critic loss: 0.501129823923111, actor loss: -0.0016089332289993762, entropy loss -0.005156169738620519\n",
      "Episode: 33, latest episode reward: 72.0, total loss: 0.48199634850025175, critic loss: 0.50214182138443, actor loss: -0.015366159565746784, entropy loss -0.0047793125733733175\n",
      "Episode: 34, latest episode reward: 117.0, total loss: 0.48871987164020536, critic loss: 0.506079226732254, actor loss: -0.012219615280628204, entropy loss -0.005139740649610758\n",
      "Episode: 35, latest episode reward: 126.0, total loss: 0.49324342906475066, critic loss: 0.5002699613571167, actor loss: -0.0017220047302544117, entropy loss -0.005304527282714844\n",
      "Episode: 36, latest episode reward: 162.0, total loss: 0.47666941285133363, critic loss: 0.5036062777042389, actor loss: -0.02189183747395873, entropy loss -0.005045030731707811\n",
      "Episode: 37, latest episode reward: 115.0, total loss: 0.48615039587020875, critic loss: 0.5026317954063415, actor loss: -0.011465422064065933, entropy loss -0.005015977984294295\n",
      "Episode: 38, latest episode reward: 78.0, total loss: 0.4924575388431549, critic loss: 0.5024351656436921, actor loss: -0.004995953850448131, entropy loss -0.004981668526306748\n",
      "Episode: 39, latest episode reward: 129.0, total loss: 0.48810062408447263, critic loss: 0.5044294118881225, actor loss: -0.011692534107714892, entropy loss -0.0046362454071640965\n",
      "Episode: 40, latest episode reward: 46.0, total loss: 0.48810062408447263, critic loss: 0.5044294118881225, actor loss: -0.011692534107714892, entropy loss -0.0046362454071640965\n",
      "Episode: 41, latest episode reward: 116.0, total loss: 0.4771933495998383, critic loss: 0.5014465034008027, actor loss: -0.01927405074238777, entropy loss -0.004979105386883021\n",
      "Episode: 42, latest episode reward: 122.0, total loss: 0.49229379296302794, critic loss: 0.5005219995975494, actor loss: -0.003888090420514345, entropy loss -0.004340113932266831\n",
      "Episode: 43, latest episode reward: 89.0, total loss: 0.47840756475925444, critic loss: 0.5005725622177124, actor loss: -0.01800995510420762, entropy loss -0.004155042301863432\n",
      "Episode: 44, latest episode reward: 47.0, total loss: 0.4993063390254974, critic loss: 0.5026314914226532, actor loss: 0.0011526836082339288, entropy loss -0.004477829905226827\n",
      "Episode: 45, latest episode reward: 33.0, total loss: 0.49047939479351044, critic loss: 0.5004981577396392, actor loss: -0.005315943993628025, entropy loss -0.004702817834913731\n",
      "Episode: 46, latest episode reward: 103.0, total loss: 0.4926008850336075, critic loss: 0.5002217411994934, actor loss: -0.0029719028621912, entropy loss -0.004648954374715686\n",
      "Episode: 47, latest episode reward: 71.0, total loss: 0.49258953630924224, critic loss: 0.5003590106964111, actor loss: -0.0030441923998296263, entropy loss -0.004725286131724715\n",
      "Episode: 48, latest episode reward: 93.0, total loss: 0.4902969241142273, critic loss: 0.500481516122818, actor loss: -0.005450845323503017, entropy loss -0.004733744682744146\n",
      "Episode: 49, latest episode reward: 35.0, total loss: 0.4902969241142273, critic loss: 0.500481516122818, actor loss: -0.005450845323503017, entropy loss -0.004733744682744146\n",
      "Episode: 50, latest episode reward: 38.0, total loss: 0.49488973915576934, critic loss: 0.500358784198761, actor loss: -0.0004856308922171593, entropy loss -0.004983413452282548\n",
      "Episode: 51, latest episode reward: 101.0, total loss: 0.4958112478256226, critic loss: 0.5009139001369476, actor loss: -0.00026810001581907273, entropy loss -0.004834556812420487\n",
      "Episode: 52, latest episode reward: 39.0, total loss: 0.4958112478256226, critic loss: 0.5009139001369476, actor loss: -0.00026810001581907273, entropy loss -0.004834556812420487\n",
      "Episode: 53, latest episode reward: 185.0, total loss: 0.49417890012264254, critic loss: 0.5024353682994842, actor loss: -0.003816972114145756, entropy loss -0.004439497925341129\n",
      "Episode: 54, latest episode reward: 63.0, total loss: 0.48686868846416476, critic loss: 0.5025227725505829, actor loss: -0.011427388293668628, entropy loss -0.0042266990058124065\n",
      "Episode: 55, latest episode reward: 133.0, total loss: 0.4907738953828812, critic loss: 0.5038866400718689, actor loss: -0.00888853957876563, entropy loss -0.004224202455952763\n",
      "Episode: 56, latest episode reward: 36.0, total loss: 0.486089888215065, critic loss: 0.5015112638473511, actor loss: -0.010791828203946353, entropy loss -0.004629545798525214\n",
      "Episode: 57, latest episode reward: 49.0, total loss: 0.486089888215065, critic loss: 0.5015112638473511, actor loss: -0.010791828203946353, entropy loss -0.004629545798525214\n",
      "Episode: 58, latest episode reward: 67.0, total loss: 0.4822341173887253, critic loss: 0.5016812860965729, actor loss: -0.014937296882271767, entropy loss -0.004509871313348413\n",
      "Episode: 59, latest episode reward: 113.0, total loss: 0.49548963010311126, critic loss: 0.5003885328769684, actor loss: -0.0003241267055273056, entropy loss -0.004574772715568542\n",
      "Episode: 60, latest episode reward: 117.0, total loss: 0.48621487319469453, critic loss: 0.5004958868026733, actor loss: -0.01020926535129547, entropy loss -0.004071744950488209\n",
      "Episode: 61, latest episode reward: 181.0, total loss: 0.48844615519046786, critic loss: 0.5027126967906952, actor loss: -0.009991041477769613, entropy loss -0.004275499284267426\n",
      "Episode: 62, latest episode reward: 64.0, total loss: 0.4842331439256668, critic loss: 0.5017311811447144, actor loss: -0.013109046965837479, entropy loss -0.004388989368453622\n",
      "Episode: 63, latest episode reward: 174.0, total loss: 0.48900762796401975, critic loss: 0.501186591386795, actor loss: -0.007669202797114849, entropy loss -0.004509760951623321\n",
      "Episode: 64, latest episode reward: 126.0, total loss: 0.4867686152458191, critic loss: 0.5012383162975311, actor loss: -0.010624229907989502, entropy loss -0.00384547458961606\n",
      "Episode: 65, latest episode reward: 84.0, total loss: 0.4926149010658264, critic loss: 0.5002722263336181, actor loss: -0.003188240434974432, entropy loss -0.004469080921262502\n",
      "Episode: 66, latest episode reward: 200.0, total loss: 0.4903721004724503, critic loss: 0.5015216410160065, actor loss: -0.007782085798680782, entropy loss -0.0033674546517431735\n",
      "Episode: 67, latest episode reward: 200.0, total loss: 0.48905347287654877, critic loss: 0.5021010339260101, actor loss: -0.009281190205365419, entropy loss -0.0037663672352209686\n",
      "Episode: 68, latest episode reward: 143.0, total loss: 0.4933429718017578, critic loss: 0.5017382681369782, actor loss: -0.004329812340438366, entropy loss -0.004065483948215843\n",
      "Episode: 69, latest episode reward: 200.0, total loss: 0.5049553066492081, critic loss: 0.5129837095737457, actor loss: -0.004885835945606232, entropy loss -0.0031425664434209465\n",
      "Episode: 70, latest episode reward: 175.0, total loss: 0.4847791999578476, critic loss: 0.5011797964572906, actor loss: -0.01264670565724373, entropy loss -0.003753888048231602\n",
      "Episode: 71, latest episode reward: 200.0, total loss: 0.48898382782936095, critic loss: 0.5022025883197785, actor loss: -0.010006741993129254, entropy loss -0.003212021687068045\n",
      "Episode: 72, latest episode reward: 197.0, total loss: 0.49221803843975065, critic loss: 0.5007987439632415, actor loss: -0.005690653715282679, entropy loss -0.0028900561621412633\n",
      "Episode: 73, latest episode reward: 191.0, total loss: 0.49586892426013945, critic loss: 0.5008294105529785, actor loss: -0.0019259851425886155, entropy loss -0.0030345011502504347\n",
      "Episode: 74, latest episode reward: 200.0, total loss: 0.4963395297527313, critic loss: 0.5007570564746857, actor loss: -0.0014741629362106322, entropy loss -0.0029433660907670855\n",
      "Episode: 75, latest episode reward: 200.0, total loss: 0.488357275724411, critic loss: 0.5013739943504334, actor loss: -0.009888795856386422, entropy loss -0.003127924236468971\n",
      "Episode: 76, latest episode reward: 200.0, total loss: 0.49161013662815095, critic loss: 0.5011860132217407, actor loss: -0.006749927252531052, entropy loss -0.002825949736870825\n",
      "Episode: 77, latest episode reward: 200.0, total loss: 0.48952108919620513, critic loss: 0.5001893639564514, actor loss: -0.00811934405937791, entropy loss -0.0025489314924925567\n",
      "Episode: 78, latest episode reward: 200.0, total loss: 0.4946442186832428, critic loss: 0.5009249210357666, actor loss: -0.0033266902901232243, entropy loss -0.002954016369767487\n",
      "Episode: 79, latest episode reward: 152.0, total loss: 0.49603556394577025, critic loss: 0.500630009174347, actor loss: -0.0019895201548933983, entropy loss -0.0026049282867461445\n",
      "Episode: 80, latest episode reward: 105.0, total loss: 0.49410935640335085, critic loss: 0.500340861082077, actor loss: -0.0036912707611918448, entropy loss -0.0025402312632650135\n",
      "Episode: 81, latest episode reward: 157.0, total loss: 0.49570886194705965, critic loss: 0.5003140568733215, actor loss: -0.001727700140327215, entropy loss -0.0028774954844266176\n",
      "Episode: 82, latest episode reward: 200.0, total loss: 0.4957291543483734, critic loss: 0.5003580808639526, actor loss: -0.0015166333876550197, entropy loss -0.0031122956657782196\n",
      "Episode: 83, latest episode reward: 200.0, total loss: 0.4805606335401535, critic loss: 0.5003054797649383, actor loss: -0.0169055400416255, entropy loss -0.002839305344969034\n",
      "Episode: 84, latest episode reward: 192.0, total loss: 0.48611517548561095, critic loss: 0.5010287284851074, actor loss: -0.011933333426713943, entropy loss -0.0029802191304042934\n",
      "Episode: 85, latest episode reward: 200.0, total loss: 0.4973029464483261, critic loss: 0.5002047896385193, actor loss: -0.00025634085759520533, entropy loss -0.0026455011451616884\n",
      "Episode: 86, latest episode reward: 191.0, total loss: 0.49472686648368835, critic loss: 0.5002488791942596, actor loss: -0.0025576040148735046, entropy loss -0.0029644151451066135\n",
      "Episode: 87, latest episode reward: 200.0, total loss: 0.4842526912689209, critic loss: 0.5020371973514557, actor loss: -0.015029723476618528, entropy loss -0.00275478505063802\n",
      "Episode: 88, latest episode reward: 184.0, total loss: 0.4937634617090225, critic loss: 0.5028013467788697, actor loss: -0.006351460888981819, entropy loss -0.0026864252518862485\n",
      "Episode: 89, latest episode reward: 170.0, total loss: 0.4919635891914368, critic loss: 0.5015496134757995, actor loss: -0.006935435626655817, entropy loss -0.0026505906134843826\n",
      "Episode: 90, latest episode reward: 193.0, total loss: 0.49240599274635316, critic loss: 0.5012197434902191, actor loss: -0.006037677638232708, entropy loss -0.002776076039299369\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-825a3836b1c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/PPO/model.py\u001b[0m in \u001b[0;36maction_value\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maction_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1943\u001b[0m     \u001b[0m_disallow_inside_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict_on_batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1944\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m       \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_batch_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1947\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1638\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 719\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3119\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3121\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3123\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step in range(num_steps):\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    values = []\n",
    "    states = []\n",
    "    dones = []\n",
    "    probs = []\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        _, policy_logits = model(state.reshape(1, -1))\n",
    "\n",
    "        action, value = model.action_value(state.reshape(1, -1))\n",
    "        new_state, reward, done, _ = env.step(action.numpy()[0])\n",
    "\n",
    "        actions.append(action)\n",
    "        values.append(value[0])\n",
    "        states.append(state)\n",
    "        dones.append(done)\n",
    "        probs.append(policy_logits)\n",
    "        episode_reward_sum += reward\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if done:\n",
    "            rewards.append(0.0)\n",
    "            state = env.reset()\n",
    "            if total_loss is not None:\n",
    "                print(f\"Episode: {episode}, latest episode reward: {episode_reward_sum}, \"\n",
    "                      f\"total loss: {np.mean(total_loss)}, critic loss: {np.mean(c_loss)}, \"\n",
    "                      f\"actor loss: {np.mean(act_loss)}, entropy loss {np.mean(ent_loss)}\")\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.scalar('rewards', episode_reward_sum, episode)\n",
    "            episode_reward_sum = 0\n",
    "\n",
    "            episode += 1\n",
    "        else:\n",
    "            rewards.append(reward)\n",
    "\n",
    "    _, next_value = model.action_value(state.reshape(1, -1))\n",
    "    discounted_rewards, advantages = get_advantages(rewards, dones, values, next_value[0])\n",
    "\n",
    "    actions = tf.squeeze(tf.stack(actions))\n",
    "    probs = tf.nn.softmax(tf.squeeze(tf.stack(probs)))\n",
    "    action_inds = tf.stack([tf.range(0, actions.shape[0]), tf.cast(actions, tf.int32)], axis=1)\n",
    "\n",
    "    total_loss = np.zeros((NUM_TRAIN_EPOCHS))\n",
    "    act_loss = np.zeros((NUM_TRAIN_EPOCHS))\n",
    "    c_loss = np.zeros(((NUM_TRAIN_EPOCHS)))\n",
    "    ent_loss = np.zeros((NUM_TRAIN_EPOCHS))\n",
    "    for epoch in range(NUM_TRAIN_EPOCHS):\n",
    "        loss_tuple = train_model(action_inds, tf.gather_nd(probs, action_inds),\n",
    "                                 states, advantages, discounted_rewards, optimizer,\n",
    "                                 ent_discount_val)\n",
    "        total_loss[epoch] = loss_tuple[0]\n",
    "        c_loss[epoch] = loss_tuple[1]\n",
    "        act_loss[epoch] = loss_tuple[2]\n",
    "        ent_loss[epoch] = loss_tuple[3]\n",
    "    ent_discount_val *= ENT_DISCOUNT_RATE\n",
    "\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar('tot_loss', np.mean(total_loss), step)\n",
    "        tf.summary.scalar('critic_loss', np.mean(c_loss), step)\n",
    "        tf.summary.scalar('actor_loss', np.mean(act_loss), step)\n",
    "        tf.summary.scalar('entropy_loss', np.mean(ent_loss), step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"saved_models/PPO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset episode\n",
    "num_test_int = 10\n",
    "global_info = []\n",
    "\n",
    "for i in range(num_test_int):\n",
    "    time, cumul_reward, done = 0, 0, False\n",
    "    old_state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        # env.render()\n",
    "        # Actor picks an action (following the policy)\n",
    "        action, value = model.action_value(state.reshape(1, -1))\n",
    "        # Retrieve new state, reward, and whether the state is terminal\n",
    "        new_state, reward, done, _ = env.step(action.numpy()[0])\n",
    "        # Update current state\n",
    "        old_state = new_state\n",
    "        cumul_reward += reward\n",
    "        time += 1\n",
    "        \n",
    "        if done: \n",
    "            global_info.append({\n",
    "                cumul_reward\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{9.0}, {10.0}, {8.0}, {12.0}, {9.0}, {12.0}, {9.0}, {10.0}, {11.0}, {9.0}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
